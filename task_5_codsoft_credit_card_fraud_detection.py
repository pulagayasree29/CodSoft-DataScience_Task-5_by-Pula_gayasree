# -*- coding: utf-8 -*-
"""Task-5_CodSoft_Credit_Card_Fraud_Detection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ATm2pzOKaU2Pwa2btfFpaKCUfuxrQ5sn

**Task-05**: CREDIT CARD FRAUD DETECTION

**Author**: Pula gayasree

**Batch**: November-December

**Domain**: Data Science

**Aim**: To perform a Credit card fraud detection that helps to identify and prevent unauthorized or fraudulent transactions using advanced algorithms and data analysis.
'''
"""

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import PowerTransformer
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
import xgboost as xgb

df = pd.read_csv('/content/Pula.gayasreecreditcard.csv')
df.head()

df.info()

df.isna().sum()

df['Class'].unique()

df.describe().T

"""'''Feature Engineering'''"""

df['hour']= round(df['Time']/3600)
df.head()

class_count = df['Class'].value_counts() #Count total number of rows of each classes.
class_count

duplicated_count = df.duplicated().sum()
duplicated_count

df = df.drop_duplicates(keep='first')

fig = plt.figure(figsize=(10, 7))
labels = class_count.index
data = class_count.values

plt.pie(data, labels=labels)
plt.title('Class Distribution')
plt.axis('equal')
plt.show()

plt.figure(figsize=(10, 5))
sns.countplot(data = df, x='Class')
plt.title('Class Distribution (Count)')
plt.xlabel('Class')
plt.ylabel('Count')
plt.show()

sns.boxplot(df['Amount'])

sns.displot(df['Amount']) #plotting distribution plot to check skewness of our data
plt.show()

df['Amount'].skew()

pt = PowerTransformer(method='yeo-johnson')   #power transformer is use for reduction of skewness of our data
df['Amount'] = pt.fit_transform(df[['Amount']])
df['Amount'].skew()

sns.distplot(df['Amount'])
plt.show()

scaler = StandardScaler()

df['Amount'] = scaler.fit_transform(df[['Amount']])

sns.boxplot(df['Amount'])

outliers = df['Amount'] > 3 # counting outliers
outliers.count()

df['Amount']=df['Amount'] < 3 #dropping outliers
class_count

sns.heatmap(df.corr())    # checking correlation of each feature to our Label
plt.show()

df.columns

x = df.drop(columns = ['Class'])
y = df['Class']

import numpy as np

# Assuming y contains NaN values
nan_indices = np.isnan(y)
x = x[~nan_indices]
y = y[~nan_indices]

# Now, you can proceed with the train-test split
from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.8, random_state=101, stratify=y)

"""'''Logistic Regression'''"""

model = LogisticRegression(class_weight = 'balanced') #creating model

model.fit(x_train,y_train)

train_pred = model.predict(x_train)

print(classification_report(y_train,train_pred))

test_pred = model.predict(x_test)

print(classification_report(y_test,test_pred))

"""'''Random classifier Forest'''

"""

model_r = RandomForestClassifier(class_weight = 'balanced')

model_r.fit(x_train,y_train)

from sklearn.ensemble import RandomForestClassifier
model_r = RandomForestClassifier()
model_r.fit(x_train, y_train)
trainpred = model_r.predict(x_train)
from sklearn.metrics import classification_report
print(classification_report(y_train, trainpred))

testpred = model_r.predict(x_test)

print(classification_report(y_test,testpred))

"""'''XGBoost'''"""

model_X = xgb.XGBClassifier()
model_X.fit(x_train,y_train)

train_x_pred = model_X.predict(x_train)

print(classification_report(y_train,train_x_pred))

test_x_pred = model_r.predict(x_test)

print(classification_report(y_test,test_x_pred))